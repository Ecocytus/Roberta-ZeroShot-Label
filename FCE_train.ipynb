{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from dataloader import DataLoader\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader('fce_train_origin', tokenizer, max_len=132, batch_size=16)\n",
    "validation_dataloader = DataLoader('fce_dev_origin', tokenizer, max_len=132, test=True, batch_size=16)\n",
    "test_dataloader = DataLoader('fce_test_origin', tokenizer, max_len=132, test=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import RobertaForTokenZeroShotClassification\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def train(model, train_dataloader):\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total=train_dataloader.total_step):\n",
    "\n",
    "        if step % 2000 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, train_dataloader.total_step, elapsed))\n",
    "            total_eval_loss, token_result, label_actual, label_predict = eval(model, validation_dataloader)\n",
    "            acc = accuracy_score(label_actual, label_predict)\n",
    "            print(' Accuracy is {}'.format(acc))\n",
    "            print(token_result)\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        result_t = model(b_input_ids,\n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        loss = result_t['loss']\n",
    "        logits = result_t['logits']\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # not clip for roberta\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_result(predicts, labels):\n",
    "    main_label = 0\n",
    "    main_correct_count = 0\n",
    "    correct_sum = 0\n",
    "    main_predicted_count = 0\n",
    "    main_total_count = 0\n",
    "    assert len(predicts) == len(labels)\n",
    "    for i in range(len(predicts)):\n",
    "        if labels[i] <= 1:\n",
    "            predicted_label = predicts[i]\n",
    "            gold_label = labels[i]\n",
    "            if gold_label == predicted_label:\n",
    "                correct_sum += 1\n",
    "            if predicted_label == main_label:\n",
    "                main_predicted_count += 1\n",
    "            if gold_label == main_label:\n",
    "                main_total_count += 1\n",
    "            if predicted_label == gold_label and gold_label == main_label:\n",
    "                main_correct_count += 1\n",
    "    p = (float(main_correct_count) / float(main_predicted_count)) if (main_predicted_count > 0) else 0.0\n",
    "    r = (float(main_correct_count) / float(main_total_count)) if (main_total_count > 0) else 0.0\n",
    "    f = (2.0 * p * r / (p + r)) if (p + r > 0.0) else 0.0\n",
    "    f05 = ((1.0 + 0.5 * 0.5) * p * r / ((0.5 * 0.5 * p) + r)) if (p + r > 0.0) else 0.0\n",
    "    return {\"p\":p, \"r\":r, \"f1\":f, \"f05\":f05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, validation_dataloader, threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    label_actual = []\n",
    "    label_predict = []\n",
    "    predict_tokens = []\n",
    "    scores = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            result_t = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels, return_mask=True)\n",
    "            loss = result_t['loss']\n",
    "            logits = result_t['logits']\n",
    "            mask = result_t['average_mask']\n",
    "\n",
    "        # predict_token = mask.max(1)[1]\n",
    "        predict_token = (mask.squeeze(1).detach().cpu().numpy() < 0).astype(int)\n",
    "        predict_token = predict_token.reshape(-1).tolist()\n",
    "        score = batch[2][:, 1:].reshape(-1).tolist()\n",
    "        predict_tokens.extend(predict_token)\n",
    "        scores.extend(score)\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        predict_ids = (logits.sigmoid().detach().cpu().numpy() > threshold).astype(int)\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        label_actual.append(label_ids)\n",
    "\n",
    "        label_predict.append(predict_ids)\n",
    "\n",
    "    return total_eval_loss, eval_result(predict_tokens, scores), np.concatenate(label_actual), np.concatenate(label_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "roberta = RobertaModel.from_pretrained('roberta-base', add_pooling_layer=False)\n",
    "# TODO: why kmin influence the recall rate that much?\n",
    "model = RobertaForTokenZeroShotClassification(roberta, num_maps=8, num_labels=1, kmax=0.1, kmin=0.1, alpha=0.3, beta=0, penalty_ratio=0.01, random_drop=0).to(device)\n",
    "model_name = 'model'\n",
    "model.update_dropout(0)\n",
    "\n",
    "optimizer = AdamW([{'params': [param for name, param in model.named_parameters() if 'roberta' not in name], 'lr': 2e-5}, \n",
    "                    {'params': model.roberta.parameters(), 'lr': 2e-5}], \n",
    "            lr=2e-5,\n",
    "            eps = 1e-6,\n",
    "            betas = (0.9, 0.98),\n",
    "            weight_decay=0.1\n",
    "            )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "total_steps = train_dataloader.total_step * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = int(total_steps / 2.5), \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef23d6a63684b13bccb844436d3af25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1705.9375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.59\n",
      "  Training epcoh took: 0:06:39\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.73\n",
      "  f1_Macro_Score: 0.72\n",
      "  f1_Micro_Score: 0.73\n",
      "{'p': 0.19550105444036553, 'r': 0.804305912596401, 'f1': 0.31454602576185986, 'f05': 0.23037699727560557}\n",
      "test result: {'p': 0.21804246321909965, 'r': 0.7850911974623315, 'f1': 0.3412969283276451, 'f05': 0.25485774304161135}\n",
      "  Validation Loss: 0.55\n",
      "  Validation took: 0:00:22\n",
      " New best val loss, save to disc.\n",
      " New best f1 score, save to disc.\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d4a035f44c4f669a60b9bae7c2769c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1705.9375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.49\n",
      "  Training epcoh took: 0:06:37\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.67\n",
      "  f1_Macro_Score: 0.67\n",
      "  f1_Micro_Score: 0.67\n",
      "{'p': 0.2544849537037037, 'r': 0.5652313624678663, 'f1': 0.3509577015163608, 'f05': 0.28592327698309494}\n",
      "test result: {'p': 0.2744718718563801, 'r': 0.5625693893735131, 'f1': 0.36894112752236324, 'f05': 0.3057916788799421}\n",
      "  Validation Loss: 0.62\n",
      "  Validation took: 0:00:21\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133cdcd8c80c4bf7b6682e5e3e5921d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1705.9375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.41\n",
      "  Training epcoh took: 0:06:37\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.76\n",
      "  f1_Macro_Score: 0.76\n",
      "  f1_Micro_Score: 0.76\n",
      "{'p': 0.23482393605530352, 'r': 0.6985861182519281, 'f1': 0.35149555375909464, 'f05': 0.27077520924671183}\n",
      "test result: {'p': 0.2614665147790635, 'r': 0.6916732751784298, 'f1': 0.37948137835015666, 'f05': 0.29861272784541437}\n",
      "  Validation Loss: 0.50\n",
      "  Validation took: 0:00:21\n",
      " New best val loss, save to disc.\n",
      " New best f1 score, save to disc.\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25353ede89164f8fbe26a685b3c2438d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1705.9375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epcoh took: 0:06:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.78\n",
      "  f1_Macro_Score: 0.77\n",
      "  f1_Micro_Score: 0.78\n",
      "{'p': 0.2378365326572498, 'r': 0.6330334190231363, 'f1': 0.3457656867046951, 'f05': 0.2717691204061362}\n",
      "test result: {'p': 0.25470129205577585, 'r': 0.6315622521808089, 'f1': 0.3630065180728383, 'f05': 0.28921718162141746}\n",
      "  Validation Loss: 0.50\n",
      "  Validation took: 0:00:21\n",
      " New best val loss, save to disc.\n",
      " New best f1 score, save to disc.\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f27a7e55a548348d73c136fd184f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1705.9375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epcoh took: 0:06:37\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.79\n",
      "  f1_Macro_Score: 0.78\n",
      "  f1_Micro_Score: 0.79\n",
      "{'p': 0.24497393894266567, 'r': 0.634318766066838, 'f1': 0.3534467323187108, 'f05': 0.2792553191489362}\n",
      "test result: {'p': 0.26310299869621906, 'r': 0.6401268834258524, 'f1': 0.37292677292677295, 'f05': 0.29823394664893227}\n",
      "  Validation Loss: 0.54\n",
      "  Validation took: 0:00:21\n",
      " New best f1 score, save to disc.\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:35:38 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "eval_loss_list = []\n",
    "f1_score_list = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    # if epoch_i <= 5:\n",
    "    #     model.update_dropout(0.1*(epoch_i))\n",
    "    # else:\n",
    "    #     model.update_dropout(0.5)\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = train(model, train_dataloader)\n",
    "\n",
    "    avg_train_loss = total_train_loss / train_dataloader.total_step         \n",
    "    \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_eval_loss, token_result, label_actual, label_predict = eval(model, validation_dataloader)\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(accuracy_score(label_actual, label_predict)))\n",
    "    print(\"  f1_Macro_Score: {0:.2f}\".format(f1_score(label_actual, label_predict,average = 'macro', zero_division=1)))\n",
    "    print(\"  f1_Micro_Score: {0:.2f}\".format(f1_score(label_actual, label_predict,average = 'micro', zero_division=1)))\n",
    "    print(token_result)\n",
    "\n",
    "    _, token_result, _, _ = eval(model, test_dataloader)\n",
    "    print(\"test result:\", token_result)\n",
    "    f1_s = f1_score(label_actual, label_predict,average = 'micro', zero_division=1)\n",
    "    \n",
    "    avg_val_loss = total_eval_loss / validation_dataloader.total_step\n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    if len(eval_loss_list) == 0 or avg_val_loss < min(eval_loss_list):\n",
    "        print(\" New best val loss, save to disc.\")\n",
    "        torch.save(model.state_dict(), \"./models/best_val_{}.pt\".format(model_name))\n",
    "\n",
    "    if len(f1_score_list) == 0 or f1_s > max(f1_score_list):\n",
    "        print(\" New best f1 score, save to disc.\")\n",
    "        torch.save(model.state_dict(), \"./models/best_f1_{}.pt\".format(model_name))\n",
    "\n",
    "    eval_loss_list.append(avg_val_loss)\n",
    "    f1_score_list.append(f1_s)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model=model.eval()\n",
    "test_model.load_state_dict(torch.load(\"./models/best_val_{}.pt\".format(model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p': 0.25470129205577585,\n",
       " 'r': 0.6315622521808089,\n",
       " 'f1': 0.3630065180728383,\n",
       " 'f05': 0.28921718162141746}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_eval_loss, token_result, label_actual, label_predict = eval(model, test_dataloader)\n",
    "token_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8055808310585381"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(label_actual, label_predict)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8505bd93e15232b680218bd613f68dd2d0ec76b40a79f48f6cb2b19121cd32c4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('azureml_py36_pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
